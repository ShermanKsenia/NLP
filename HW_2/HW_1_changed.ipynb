{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cf8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from fake_useragent import UserAgent\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a35a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent(verify_ssl=False)\n",
    "headers = {'User-Agent': ua.random}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5030a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f9d257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_tag = {\n",
    "    'SPRO': 'PRON', 'ADVPRO': 'ADV', 'APRO': 'PRON',\n",
    "    'NPRO': 'PRON', 'DET': 'PRON', 'S': 'NOUN',\n",
    "    'PROPN': 'NOUN', 'V': 'VERB', 'INFN': 'VERB',\n",
    "    'PRTS': 'PRT', 'PRTF': 'PRT', 'ADVB': 'ADV',\n",
    "    'ANUM': 'NUM', 'NUMR': 'NUM', 'ADJF': 'ADJ',\n",
    "    'ADJS': 'ADJ', 'PR': 'ADP', 'CCONJ': 'CONJ',\n",
    "    'SCONJ': 'CONJ', 'PRCL': 'PART', 'AUX': 'VERB',\n",
    "    'PREP': 'ADP', 'A': 'ADJ'\n",
    "}\n",
    "is_cyrilic = re.compile(r'[а-яА-ЯёЁ]+?\\b')\n",
    "\n",
    "def mystem_tag(sent):\n",
    "    m = Mystem()\n",
    "    poses = []\n",
    "    analysis = m.analyze(sent)\n",
    "    for word in analysis:\n",
    "        if is_cyrilic.match(word['text']):\n",
    "            try:\n",
    "                feats = word['analysis'][0]['gr']\n",
    "                pos = feats.split('=')[0].split(',')[0]\n",
    "            except:\n",
    "                pos = 'X'\n",
    "            if pos == 'V':\n",
    "                if 'деепр' in feats:\n",
    "                    pos = 'GRND'\n",
    "                elif 'прич' in feats:\n",
    "                    pos = 'PRT'\n",
    "            if pos in ['A', 'ADV']:\n",
    "                if 'срав' in feats:\n",
    "                    pos = 'COMP'\n",
    "            if pos in tag_to_tag.keys():\n",
    "                poses.append(tag_to_tag[pos])\n",
    "            else:\n",
    "                poses.append(pos)\n",
    "    return poses\n",
    "\n",
    "def chunker(pattern, text):\n",
    "    m = Mystem()\n",
    "    result = []\n",
    "    pattern = pattern.split('+')\n",
    "    lemmas = [lemma for lemma in m.lemmatize(text) if is_cyrilic.match(lemma)]\n",
    "    poses = mystem_tag(text)\n",
    "    result_sentence = [(l, p) for l, p in zip(lemmas, poses)]\n",
    "    all_ngrams = ngrams(result_sentence, len(pattern))\n",
    "    word_ngrams = list(ngrams(lemmas, len(pattern)))\n",
    "    if len(lemmas) != len(poses):\n",
    "        return []\n",
    "    for j, ngram in enumerate(all_ngrams):\n",
    "        cnt = 0\n",
    "        for i, word in enumerate(pattern):\n",
    "            if word in ngram[i]:\n",
    "                cnt += 1\n",
    "        if cnt == len(pattern):\n",
    "            result.append(' '.join(word_ngrams[j]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee5184",
   "metadata": {},
   "source": [
    "### 1. Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd67b6",
   "metadata": {},
   "source": [
    "Я выбрала сайт https://www.turpravda.com/tn/top-hotels.html с отзывами на отели Туниса. Оттуда я взяла страницы с отелями, которые оценивались на 9-10, 7, 6 и 1-5 баллов в среднем. На каждой странице максимум 25 отелей. \n",
    "\n",
    "Так как страницы с самими отзывами позволяют просматривать новые отзывы только по нажатию специальной кнопки, пришлось брать только по 10 первых отзывов с каждой страницы, так как я не нашла функционала, с помощью которого можно достать больше отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b7bff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_comments(page_url):\n",
    "    global df_comments\n",
    "    \n",
    "    session = requests.session()\n",
    "    req = session.get(page_url, headers=headers)\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page)\n",
    "    \n",
    "    for comment in soup.find_all('div', {'class': 'ans_body'}):\n",
    "        mark = comment.find('span', {'class': 'value'})\n",
    "        if mark:\n",
    "            comment_text = comment.find('span', {'class': 'all-text'}).text\n",
    "            mark = float(mark.text[-4:])\n",
    "            df_comments = df_comments.append({'comment': comment_text, \n",
    "                                              'mark': mark,\n",
    "                                              'url': page_url}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087f5306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:09<00:00,  1.59s/it]\n",
      "100%|██████████| 6/6 [00:07<00:00,  1.31s/it]\n",
      "100%|██████████| 6/6 [00:08<00:00,  1.38s/it]\n",
      "100%|██████████| 6/6 [00:08<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "df_comments = pd.DataFrame(columns=['comment', 'mark', 'url'])\n",
    "rates = [9, 7, 5, '5&p=2']\n",
    "\n",
    "for i in rates:\n",
    "    page_url = f'https://www.turpravda.com/tn/top-hotels.html?rte%5B%5D={i}'\n",
    "    session = requests.session()\n",
    "    req = session.get(page_url, headers=headers)\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page)\n",
    "\n",
    "    all_hrefs = soup.find_all('a', {'class': 'hotel-name-title'})\n",
    "    for href in tqdm(all_hrefs[:6]):\n",
    "        link = href.get('href')\n",
    "        get_comments('https://www.turpravda.com' + link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffca56c",
   "metadata": {},
   "source": [
    "Таким образом, у меня получилось почти равное количество положительных и отрицательных отзывов. К отрицательным я относила те отзывы, в которых оценка от 1 до 5, включительно. Остальные я относила к положительным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f60ee2da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    111\n",
       "0     86\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments['sentiment'] = df_comments['mark'].apply(lambda x: 1 if x > 5 else 0)\n",
    "df_comments['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc14771",
   "metadata": {},
   "source": [
    "### 2. Создание словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3969075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "sw = stopwords.words('russian')\n",
    "def clean_text(text):\n",
    "    tokens = []\n",
    "    for word in word_tokenize(text):\n",
    "        if is_cyrilic.search(word):\n",
    "            if word not in sw:\n",
    "                tokens.append(morph.parse(word.lower())[0].normal_form)\n",
    "    return tokens\n",
    "\n",
    "def clean_text_not_sw(text):\n",
    "    tokens = []\n",
    "    for word in word_tokenize(text):\n",
    "        if is_cyrilic.search(word):\n",
    "            tokens.append(morph.parse(word.lower())[0].normal_form)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b4cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments['tokens'] = df_comments['comment'].apply(clean_text)\n",
    "df_comments['clean_comment'] = df_comments['tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7312217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_comments['comment']\n",
    "y = df_comments['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567fd7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b550b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pos = y==1\n",
    "X_train = pd.Series(X_train)\n",
    "positive_comments = X_train[mask_pos].tolist()\n",
    "negative_comments = X_train[~mask_pos].tolist()\n",
    "\n",
    "positive_corpus = []\n",
    "negative_corpus = []\n",
    "\n",
    "for t in positive_comments:\n",
    "    tokens = clean_text(t)\n",
    "    positive_corpus.extend(tokens)\n",
    "for t in negative_comments:\n",
    "    tokens = clean_text(t)\n",
    "    negative_corpus.extend(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b17b8bb",
   "metadata": {},
   "source": [
    "### Применение чанкера"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915ac75",
   "metadata": {},
   "source": [
    "Я выбрала три шаблона: 'быть+ADJ', 'ADJ+еда', 'не+быть+NOUN'. Я предположила, что они могут помочь определиться с тональностью, так как часто встречаются в отзывах и обычно имеют определённый окрас, не нейтральны. Например, _был чистым_, _отвратительная еда_, _не было полотенец_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f23f4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [03:34<00:00,  2.46s/it]\n",
      "100%|██████████| 70/70 [02:53<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [03:38<00:00,  2.51s/it]\n",
      "100%|██████████| 70/70 [02:53<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [03:32<00:00,  2.44s/it]\n",
      "100%|██████████| 70/70 [02:49<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bigrams_pos = []\n",
    "bigrams_neg = []\n",
    "patterns = ['быть+ADJ', 'ADJ+еда', 'не+быть+NOUN']\n",
    "for pattern in patterns:\n",
    "    for comment in tqdm(positive_comments):\n",
    "        new_tokens = chunker(pattern, comment)\n",
    "        bigrams_pos.extend(new_tokens)\n",
    "\n",
    "    for comment in tqdm(negative_comments):\n",
    "        new_tokens = chunker(pattern, comment)\n",
    "        bigrams_neg.extend(new_tokens)\n",
    "    print(len(bigrams_neg), len(bigrams_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07acc74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only positive:\n",
      "{'рыба', 'кто', 'ещё', 'душа', 'красивый', 'язык', 'английский', 'этот', 'бесплатно', 'отличный', 'ездить', 'расположить', 'приятный', 'немного', 'сахар', 'супер', 'возле', 'хотеться', 'новый', 'впечатление', 'вечер', 'центр', 'прекрасный', 'правда', 'дорога', 'зелёный', 'медина', 'сейф', 'решить', 'какой-то', 'египет', 'купить', 'чаевой', 'больший', 'небольшой', 'у', 'увидеть', 'замечательный', 'огромный', 'оставить', 'около', 'д.', 'аэропорт', 'ваш', 'доллар', 'ждать', 'отдельный', 'поменять', 'покупать', 'конец', 'страна', 'любой', 'знать', 'вопрос', 'мясо', 'кормить', 'достаточно', 'поездка', 'кухня', 'аниматор', 'шоу', 'город', 'турция', 'кстати'}\n",
      "Only negative:\n",
      "{'за', 'приличный', 'заселение', 'неделя', 'поздний', 'второй', 'кондиционер', 'рубль', 'стакан', 'к.', 'рекомендовать', 'ресепшен', 'бельё', 'утром', 'быстро', 'сутки', 'нормально', 'стол', 'лежать', 'тарелка', 'детский', 'минус', 'тур', 'делать', 'кофе', 'ужасный', 'комната', 'постельный', 'мусор', 'але', 'официант', 'кровать', 'принести', 'очередь', 'це', 'туалет', 'телевизор', 'сервис', 'старый', 'що', 'поехать', 'як', 'отвратительный', 'грязный', 'включить', 'чай', 'із', 'отсутствовать', 'столовая', 'з', 'плохой', 'раз', 'посуда', 'дело', 'нет', 'мало', 'зато', 'стоять', 'интернет', 'два', 'берег', 'нечего', 'думать', 'плохо'}\n"
     ]
    }
   ],
   "source": [
    "cnt_pos = Counter(positive_corpus).most_common(200)\n",
    "cnt_neg = Counter(negative_corpus).most_common(200)\n",
    "\n",
    "set_pos = set(dict(cnt_pos).keys())\n",
    "set_pos_all = set_pos.union(set(bigrams_pos))\n",
    "set_neg = set(dict(cnt_neg).keys())\n",
    "set_neg_all = set_neg.union(set(bigrams_neg))\n",
    "print('Only positive:')\n",
    "print(set_pos-set_neg)\n",
    "print('Only negative:')\n",
    "print(set_neg-set_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "286d7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(comments, pos_dict, neg_dict):\n",
    "    cnt_pos = 0\n",
    "    cnt_neg = 0\n",
    "    result = []\n",
    "    for i, comment in enumerate(comments):\n",
    "        tokens = clean_text_not_sw(comment)\n",
    "        ngr = list(ngrams(tokens, 2))\n",
    "        tokens.extend(ngr)\n",
    "        for token in tokens:\n",
    "            if token in pos_dict:\n",
    "                cnt_pos += 1\n",
    "            elif token in neg_dict:\n",
    "                cnt_neg += 1\n",
    "        if cnt_neg > cnt_pos:\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "        cnt_pos = 0\n",
    "        cnt_neg = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a0adc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_count(y_pred, y_test):\n",
    "    cnt = 0\n",
    "    for p, t in zip(y_pred, y_test):\n",
    "        if p == t:\n",
    "            cnt += 1\n",
    "    return cnt / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42f554c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_pos = set_pos - set_neg\n",
    "only_neg = set_neg - set_pos\n",
    "y_pred = sentiment(X_test, only_pos, only_neg)\n",
    "accuracy_count(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8da4455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_pos = set_pos_all - set_neg_all\n",
    "only_neg = set_neg_all - set_pos_all\n",
    "y_pred = sentiment(X_test, only_pos, only_neg)\n",
    "accuracy_count(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1549b43",
   "metadata": {},
   "source": [
    "Но результат не изменился, скорее всего из-за того, что отзывов мало, следовательно, маленькая вероятность, что встретятся одинаковые н-граммы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da133789",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
