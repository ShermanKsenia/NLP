{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iatn8mPa-BTG",
        "outputId": "9bb44712-9659-49e2-976a-f8123905ddef"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from natasha import Segmenter, Doc, NewsMorphTagger, NewsEmbedding\n",
        "from pymystem3 import Mystem\n",
        "from nltk import word_tokenize\n",
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Я выбрала в качестве сложных слов:\n",
        "1. Слова, которые считаются окказиональными, появляющимися в обычной речи, но считающимися ошибочными (поуезжать, пробудясь, принесть и тд). Такие формы скорее не находятся в словаре и могли не встречаться модели.\n",
        "2. Формы, которые омонимичны, но относятся к разным частям речи в зависимости от контекста (светило, печь, мыла). Такие формы интересны с точки зрения понимания модели контекста предложения (но это относится только к моделям, определяющим контекст)\n",
        "3. Слова с неизвестными корнями, но форму которых можно понять по морфологическим признакам (зафрендить, бомжатник). Аналогично первой группе слов, в данном случае слова скорее неизвестны модели.\n",
        "4. Аббревиатуры. Они тоже могут не быть в словаре модели и, в отличие от п.1 и 3, не имеют морфологических характеристик, которые могут помочь предсказать часть речи. \n",
        "\n",
        "Так как тэгсеты различаются, я решила свести все возможные метки к наиболее узкому сету: NOUN, VERB, ADJ, ADV, GRND (деепричастие), PRT (причастие), ADP (предлог), PART (частица), CONJ, NUM, PRON. Перевод из одного формата в другой записан в ячейке ниже. Я решила объединить краткие и полные формы прилагательных под одним тэгом, так как важнее, чтобы модель угадала, что это вообще прилагательное, когда как краткость и полнота скорее второстепенная характеристика, аналогично с разными видами местоимений и причастий. Вспомогательный глагол я относила к общему классу глаголов по той же причине. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "tag_to_tag = {\n",
        "    'SPRO': 'PRON', 'ADVPRO': 'ADV', 'APRO': 'PRON',\n",
        "    'NPRO': 'PRON', 'DET': 'PRON', 'S': 'NOUN',\n",
        "    'PROPN': 'NOUN', 'V': 'VERB', 'INFN': 'VERB',\n",
        "    'PRTS': 'PRT', 'PRTF': 'PRT', 'ADVB': 'ADV',\n",
        "    'ANUM': 'NUM', 'NUMR': 'NUM', 'ADJF': 'ADJ',\n",
        "    'ADJS': 'ADJ', 'PR': 'ADP', 'CCONJ': 'CONJ',\n",
        "    'SCONJ': 'CONJ', 'PRCL': 'PART', 'AUX': 'VERB',\n",
        "    'PREP': 'ADP', 'A': 'ADJ'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Я выбрала теггеры pymorphy, natasha и mystem. \n",
        "Я сделала словарь, в котором ключ - это предложение, а значение это вложенный словарь, в котором записаны отдельно токены, правильные части речи и предсказания каждой из исследуемых моделей. Конвертация предсказаний в теги ручной разметки происходит внутри функций."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "sents = {}\n",
        "re_sent = re.compile(r'#sent = (.+?)\\n')\n",
        "with open('morph.txt', 'r') as f:\n",
        "    txt = f.readlines()\n",
        "\n",
        "for line in txt:\n",
        "    if line.startswith('#sent'):\n",
        "        sent = re_sent.search(line).group(1)\n",
        "        sents[sent] = {'words': [], 'pos': []}\n",
        "    elif line != '\\n':\n",
        "        word, pos = line.split()\n",
        "        sents[sent]['words'].append(word.strip())\n",
        "        sents[sent]['pos'].append(pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "204\n"
          ]
        }
      ],
      "source": [
        "word_cnt = 0\n",
        "for v in sents.values():\n",
        "    word_cnt += len(v['words'])\n",
        "print(word_cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "morph = MorphAnalyzer()\n",
        "def pymorhy_tag(sents):\n",
        "    for k, v in sents.items():\n",
        "        poses = []\n",
        "        for word in v['words']:\n",
        "            p = morph.parse(word)\n",
        "            pos = p[0].tag.POS\n",
        "            if pos in tag_to_tag.keys():\n",
        "                poses.append(tag_to_tag[pos])\n",
        "            else:\n",
        "                poses.append(pos)\n",
        "        sents[k]['pymorphy'] = poses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def natasha_tag(sents):\n",
        "    segmenter = Segmenter()\n",
        "    emb = NewsEmbedding()\n",
        "    morph_tagger = NewsMorphTagger(emb)\n",
        "    for k, v in sents.items():\n",
        "        poses = []\n",
        "        doc = Doc(k)\n",
        "        doc.segment(segmenter)\n",
        "        doc.tag_morph(morph_tagger)\n",
        "        for token in doc.tokens:\n",
        "            pos = token.pos\n",
        "            if pos == 'VERB':\n",
        "                if token.feats['VerbForm'] == 'Conv':\n",
        "                    pos = 'GRND'\n",
        "                elif token.feats['VerbForm'] == 'Part':\n",
        "                    pos = 'PRT'\n",
        "            if pos in ['ADJ', 'ADV']:\n",
        "                if token.feats['Degree'] == 'Cmp':\n",
        "                    pos = 'COMP'\n",
        "            if pos != 'PUNCT':\n",
        "                if pos in tag_to_tag.keys():\n",
        "                    poses.append(tag_to_tag[pos])\n",
        "                else:\n",
        "                    poses.append(pos)\n",
        "        sents[k]['natasha'] = poses "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mystem_tag(sents):\n",
        "    m = Mystem()\n",
        "    for k, v in sents.items():\n",
        "        poses = []\n",
        "        analysis = m.analyze(k)\n",
        "        for word in analysis:\n",
        "            if word['text'].isalpha():\n",
        "                feats = word['analysis'][0]['gr']\n",
        "                pos = feats.split('=')[0].split(',')[0]\n",
        "                if pos == 'V':\n",
        "                    if 'деепр' in feats:\n",
        "                        pos = 'GRND'\n",
        "                    elif 'прич' in feats:\n",
        "                        pos = 'PRT'\n",
        "                if pos in ['A', 'ADV']:\n",
        "                    if 'срав' in feats:\n",
        "                        pos = 'COMP'\n",
        "                if pos in tag_to_tag.keys():\n",
        "                    poses.append(tag_to_tag[pos])\n",
        "                else:\n",
        "                    poses.append(pos)\n",
        "        sents[k]['mystem'] = poses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "pymorhy_tag(sents)\n",
        "natasha_tag(sents)\n",
        "mystem_tag(sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задание 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_accuracy(tagger_name):\n",
        "    acc_count = 0\n",
        "    all_pos = 0\n",
        "    errors = []\n",
        "    for v in sents.values():\n",
        "        for y_true, y_pred, word in zip(v['pos'], v[tagger_name], v['words']):\n",
        "            if y_true == str(y_pred):\n",
        "                acc_count += 1\n",
        "            else:\n",
        "                errors.append([word, y_true, y_pred])\n",
        "            all_pos += 1\n",
        "    return acc_count / all_pos, errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pymorphy: 0.8676470588235294\n",
            "natasha: 0.8078817733990148\n",
            "mystem: 0.9458128078817734\n"
          ]
        }
      ],
      "source": [
        "py_acc, py_errs = count_accuracy('pymorphy')\n",
        "nat_acc, nat_errs = count_accuracy('natasha')\n",
        "my_acc, my_errs = count_accuracy('mystem')\n",
        "\n",
        "print('pymorphy:', py_acc)\n",
        "print('natasha:', nat_acc)\n",
        "print('mystem:', my_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Лучшим теггером стал mystem!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Задание 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "На вход чанкеру подаётся шаблон, по которому нужно искать н-грамму, и предложение. Предложение делится на леммы и части речи, а после составляется список, в котором один элемент - это кортеж ('лемма', 'часть речи'). Из этого списка составляются биграммы и ищется нужный паттерн. Если паттерн найден, то берётся индекс биграммы и выводится сочетание лемм, соотвествующее шаблону."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "is_cyrilic = re.compile(r'[а-яА-ЯёЁ]+?\\b')\n",
        "def mystem_tag(sent):\n",
        "    m = Mystem()\n",
        "    poses = []\n",
        "    analysis = m.analyze(sent)\n",
        "    for word in analysis:\n",
        "        if is_cyrilic.match(word['text']):\n",
        "            try:\n",
        "                feats = word['analysis'][0]['gr']\n",
        "                pos = feats.split('=')[0].split(',')[0]\n",
        "            except:\n",
        "                pos = 'X'\n",
        "            if pos == 'V':\n",
        "                if 'деепр' in feats:\n",
        "                    pos = 'GRND'\n",
        "                elif 'прич' in feats:\n",
        "                    pos = 'PRT'\n",
        "            if pos in ['A', 'ADV']:\n",
        "                if 'срав' in feats:\n",
        "                    pos = 'COMP'\n",
        "            if pos in tag_to_tag.keys():\n",
        "                poses.append(tag_to_tag[pos])\n",
        "            else:\n",
        "                poses.append(pos)\n",
        "    return poses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "patterns = ['грязный+NOUN', 'ADJ+NOUN', 'NOUN+быть+ADJ']\n",
        "def chunker(pattern, text):\n",
        "    m = Mystem()\n",
        "    result = []\n",
        "    pattern = pattern.split('+')\n",
        "    lemmas = [lemma for lemma in m.lemmatize(text) if is_cyrilic.match(lemma)]\n",
        "    poses = mystem_tag(text)\n",
        "    result_sentence = [(l, p) for l, p in zip(lemmas, poses)]\n",
        "    all_ngrams = list(ngrams(result_sentence, len(pattern)))\n",
        "    word_ngrams = list(ngrams(lemmas, len(pattern)))\n",
        "    for j, ngram in enumerate(all_ngrams):\n",
        "        cnt = 0\n",
        "        for i, word in enumerate(pattern):\n",
        "            if word in ngram[i]:\n",
        "                cnt += 1\n",
        "        if cnt == len(pattern):\n",
        "            result.append(' '.join(word_ngrams[j]))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['розовый кукла']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunker('ADJ+NOUN', 'Я хотела розовую куклу')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
